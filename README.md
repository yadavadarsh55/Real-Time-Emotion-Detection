# EngageAI (Real-Time-Emotion-Detection)

[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE.md)
[![Python](https://img.shields.io/badge/Python-3.7+-blue.svg)](https://www.python.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)](https://www.tensorflow.org/)
[![MediaPipe](https://img.shields.io/badge/MediaPipe-0.8+-green.svg)](https://mediapipe.dev/)
[![Flask](https://img.shields.io/badge/Flask-2.x-lightgrey.svg)](https://flask.palletsprojects.com/)
[![Chrome Extension](https://img.shields.io/badge/Chrome-Extension-yellow.svg)](https://developer.chrome.com/docs/extensions/)

**Real-Time-Emotion-Detection** is an AI-driven, real-time facial emotion detection and engagement analysis tool. It utilizes deep learning and computer vision to assess user engagement via webcam or browser-based interfaces.

---

## ğŸŒ Overview

This project captures real-time facial expressions and processes them to detect seven distinct emotions. It then uses these emotional cues, along with head movement, gaze direction, and smile detection, to determine an overall engagement score.

![Emotion Detection Demo](https://via.placeholder.com/640x360.png?text=Emotion+Detection+Demo)

---

## âœ¨ Features

- ğŸ˜Š **Emotion Recognition**: Detects 7 emotions: Angry, Disgusted, Fearful, Happy, Neutral, Sad, Surprised
- ğŸ§  **Engagement Scoring**: Combines emotion, smile, gaze, and head movement into a final score
- ğŸ˜ **Real-Time Analysis**: Processes webcam video live with MediaPipe and OpenCV
- ğŸŒ **Flask API**: Enables easy integration with web and mobile apps
- ğŸ“² **Chrome Extension**: Allows engagement monitoring during online meetings
- ğŸ“ **Training Support**: Includes training and testing scripts for custom models

---

## ğŸ§  Emotions & Scoring

| Emotion     | Score |
|-------------|-------|
| Angry       | 0.6   |
| Disgusted   | 0.1   |
| Fearful     | 0.4   |
| Happy       | 1.0   |
| Neutral     | 0.5 / 0.8 |
| Sad         | 0.2   |
| Surprised   | 0.8 / 0.9 |

---

## ğŸ“š Technologies Used

- **Python** â€“ Core language
- **TensorFlow / Keras** â€“ Deep learning model for emotion classification
- **OpenCV** â€“ Video and image processing
- **MediaPipe** â€“ Facial landmark detection
- **Flask** â€“ Backend API framework
- **NumPy** â€“ Numerical computing
- **Chrome Extensions API** â€“ Browser integration

---

## âš™ï¸ Installation

### Backend Setup
```bash
git clone https://github.com/yadavadarsh55/Real-Time-Emotion-Detection.git
cd Real-Time-Emotion-Detection
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### Chrome Extension Setup
1. Go to `chrome://extensions/`
2. Enable "Developer mode"
3. Click "Load unpacked" and choose the `extension/` folder

---

## ğŸ”§ Usage

### Standalone Webcam Mode
```bash
python main.py
```

### Flask API Server
```bash
python server.py
```

- `POST /analyze` â€“ Analyze a base64 image
- `GET /status` â€“ Server health check

### Chrome Extension (EngageAI)

- Start server (`python server.py`)
- Click the extension icon
- Press "Activate"
- Join a video meeting (Meet, Zoom, etc.)

Displays:
- Detected emotion
- Emotion score
- Engagement level

---

## ğŸ“„ API Reference

### POST `/analyze`
Analyze facial features from a base64 image.

**Request:**
```json
{
  "frame": "base64-encoded-image"
}
```
**Response:**
```json
{
  "faces": [
    {
      "emotion": "Happy",
      "emotion_score": 1.0,
      "smile_score": 1.0,
      "head_score": 0.6,
      "gaze_score": 1.0,
      "engagement_score": 0.88,
      "engagement_level": "Fully Engaged",
      "bounding_box": {
        "x": 120,
        "y": 80,
        "width": 200,
        "height": 200
      }
    }
  ]
}
```

### GET `/status`
Returns server and model health.

---

## ğŸ“ˆ Training Your Own Model

Prepare your dataset:
```
data/
â”œâ”€â”€ train/
â”œâ”€â”€ validation/
â”œâ”€â”€ test/
    â””â”€â”€ Angry/, Happy/, etc.
```
Train:
```bash
python train_model.py
```
Test:
```bash
python test_model.py
```

---

## ğŸ“Š Engagement Metrics

| Metric         | Weight |
|----------------|--------|
| Emotion Score  | 30%    |
| Smile Score    | 10%    |
| Head Position  | 40%    |
| Gaze Direction | 20%    |

**Levels:**
- Fully Engaged â‰¥ 60%
- Partially Engaged: 50â€“59.9%
- Not Engaged < 50%

---

## ğŸ—‚ï¸ Project Structure

```
Real-Time-Emotion-Detection/
â”œâ”€â”€ model/                 # Pretrained models
â”œâ”€â”€ extension/             # Chrome Extension
â”œâ”€â”€ emotion_score.py       # Emotion utils
â”œâ”€â”€ main.py                # Webcam detection
â”œâ”€â”€ server.py              # Flask API
â”œâ”€â”€ train_model.py         # Training
â”œâ”€â”€ test_model.py          # Evaluation
â”œâ”€â”€ LICENSE.md
â””â”€â”€ README.md
```

---

## ğŸ‘œ Chrome Extension Breakdown

- `background.js` â€“ Background scripts
- `content.js` â€“ Injected scripts
- `popup.html` â€“ User UI
- `popup.js` â€“ Functionality
- Requires: `activeTab`, `storage`, `nativeMessaging`, `scripting`

---

## ğŸ‘¤ Contributors

- **Adarsh Yadav** â€“ [@yadavadarsh55](https://github.com/yadavadarsh55)

---

## ğŸ™ Acknowledgments

- FER2013 dataset
- MediaPipe by Google
- TensorFlow team
- Chrome Extension APIs

---

## ğŸ“„ License

MIT License â€“ see [LICENSE.md](LICENSE.md)

